% !TeX root = ../main.tex

\section* {Dirichlet Process}

\paragraph{Idea}

The Dirichlet Process is the idea of drawing a GMM from a meta-distribution, which can model infinite Gaussian mixtures.

 We need a fitting algorithm (\textbf{Gibbs sampling}) that fits a GMM to the data independent of the number of components. This is based on the \textbf{Chinese restaurant process}, which provides a constructive way of sampling from a Dirichlet process.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{chinese_restaurant_process}
  % \caption{There are tables in a restaurant (our mixture components). Whenever a new customer comes in (a sample), it chooses the table it most relates to. When the customer is unsatisfied with current offering, he can open up a new table (\href{https://en.wikipedia.org/wiki/Chinese_restaurant_process}{Check the explaination form wikipedia}).}
\end{figure}

\paragraph{This CRP extends a ``normal'' GMM:}
\begin{enumerate}
	\item If a customer prefers to sit at a new table, this is possible we can add arbitrary many tables
	\item The more people sit on a table  the more attractive it is to the new customers (rich get richer)
\end{enumerate}

\paragraph{Gibbs sampling:}
A straight forward (probabilistic\footnote{because we sample from the list}) clustering algorithm based on the CRP

\begin{enumerate}
    \item Init: assign each sample to a cluster (e.g. randomly, or do \(k-\)means)
    \item Randomly select a sample \(x_i\) from the data
    \item Compute a affinity of \(x_i\) to each table: \(t_i = \frac{N_i}{N+\alpha} \mathcal{N}(x_i, \mu_i, \Sigma_i)\) (Gaussian distance of \(x_i\) to \(\mathcal(\mu_i, \Sigma_i)\)), where \(N_i\) is the number of customers on the table (number of samples assigned to \(\mathcal{N}(\mu_i, \Sigma_i)\) and \(N\) is the total number of samples, \(\alpha\) is an "expansion parameter"
    \item Compute affinity to a new table \(t_0 = \frac{\alpha}{N + \alpha} \cdot \mathcal{N}(x_i, \mu_0, \Sigma_0)\)
    \item Collect all affinities \(t_0, \dots, t_T\) in a list, normalize sum to one
    \item Sample from that list a table assignment (all affinities are interpreted as a PDF from which we sample)
    \item Recompute \(\mu_i, \Sigma_i\) for the assigned table
    \item Goto 2, stop after certain number of iterations
\end{enumerate}

\paragraph{Back to the top-down view:}
Using the CRP, we are effectively drawing a GMM using a Dirichlet process. Implicitly, the mixture weights $\beta_i$ match a $Beta(\alpha)$ distribution and the normal distributions are drawn from a hyper-distribution, i.e. $\vartheta_i \sim H(\lambda)$

\paragraph{Stick breaking process:} \href{https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process}{(check the wiki-page)}
\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{stick_breaking_process}
    \caption{Illustration of a $Beta$ distribution}
  \end{minipage}
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{Beta_distribution}
    \caption{Qualitatively, this is what the $Beta$ distribution looks like}
  \end{minipage}
\end{figure}

The expansion parameter $\alpha$ is a prior that influences the number of clusters that will be created.

\paragraph{Observation 1:}
The larger the expansion parameter $\alpha$ is, the more likely it is to draw a number that is close to $0$ form $Beta(1,\alpha)$

The number that has been drawn form $Beta(1,\alpha)$ is used in the stick breaking process to determine the weight of the i-th mixture component $\beta_i$

\paragraph{Observation 2:}
The number that has been drawn from \(Beta(1,\alpha)\) is used in the stick breaking process to determine the weight of the \(i\)-th mixture component \(\beta_i\).

\paragraph{Stick breaking process (formalized):}

\begin{equation*}
	b_i \sim Beta(1,\alpha)
\end{equation*}

\begin{equation*}
	\beta_i = b_i \dot \prod_{i=l}^{i-1}(1-b_l) = b_i (1 - \sum_{l=1}^{i-1} \beta_l)
\end{equation*}
